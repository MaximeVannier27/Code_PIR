{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Clustering based Intrusion Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "__author__ = \"Alec F.\"\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from core.models.network import AdaptiveClustering\n",
    "from core.utils.misc import extend_dataset\n",
    "from core.utils import Dataset\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from core.models.RandomForest import RandomForest\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support as prf, accuracy_score\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from codecarbon import track_emissions\n",
    "from datetime import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers\n",
    "\n",
    "We use these helpers functions throughout all our experiments for simplify our main code and improve its readability. In all our experiments, our datasets are separated to approximately obtain a 70/30 train-test split from randomly sampled data. We use the pickle library to save our trained model and defined a set of metrics that are commonly used for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path_train, dataset_path_test):\n",
    "    global categories\n",
    "    if not os.path.exists(dataset_path_train):\n",
    "        print(f\"Dataset not found: {dataset_path_train}\")\n",
    "        sys.exit(1)\n",
    "    if not os.path.exists(dataset_path_test):\n",
    "        print(f\"Dataset not found: {dataset_path_test}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    train_df = pd.read_csv(dataset_path_train, index_col=False, sep=\",\")\n",
    "    test_df = pd.read_csv(dataset_path_test, index_col=False, sep=\",\")\n",
    "\n",
    "    \n",
    "    # For binary classification, uncomment the following line\n",
    "    train_df['Label.'] = np.where(train_df['Label.'] == 0, \"Normal\", \"Attack\")\n",
    "    test_df['Label.'] = np.where(test_df['Label.'] == 0, \"Normal\", \"Attack\")\n",
    "    \n",
    "\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def save_model(rf, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pkl.dump(rf, f)\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        m = pkl.load(f)\n",
    "    return m\n",
    "\n",
    "\n",
    "class metrics(object):\n",
    "\n",
    "    def __init__(self, tp=None, tn=None, fp=None, fn=None):\n",
    "        super(metrics, self).__init__()\n",
    "\n",
    "        self.tp, self.tn, self.fp, self.fn = tp, tn, fp, fn\n",
    "\n",
    "        self.metrics = {}\n",
    "\n",
    "    def accuracy(self):\n",
    "        return (self.tp + self.tn) / (self.tp + self.tn + self.fp + self.fn)\n",
    "\n",
    "    def detection_rate(self):\n",
    "        return self.tp / (self.tp + self.fn)\n",
    "\n",
    "    def false_alarm_rate(self):\n",
    "        return self.fp / (self.fp + self.tn)\n",
    "\n",
    "    def precision(self):\n",
    "        return self.tp / (self.tp + self.fp)\n",
    "\n",
    "    def f1(self):\n",
    "        prec = self.precision()\n",
    "        rec = self.detection_rate()\n",
    "        return 0 if (prec + rec) == 0 else 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        self.metrics = {\"Acc\": self.accuracy(), \"DR/Recall\": self.detection_rate(), \"FAR\": self.false_alarm_rate(),\n",
    "                        \"PRECISION\": self.precision(), \"F1 SCORE\": self.f1()}\n",
    "        return self.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Clustering - training function\n",
    "\n",
    "We train our Adaptive Clustering network by following the exact settings provided in the paper.\n",
    "However, we also implement an early stop mechanism allowing us to stop the training as soon as we achieve an acceptable loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@track_emissions\n",
    "def train(X, y,j,dic_loss, lr=1e-4):\n",
    "\n",
    "    dic_loss[f\"entrainement_{j}\"] = []\n",
    "\n",
    "    batch_size = 1024\n",
    "    print(\"Adaptive training...\")\n",
    "    model_ = AdaptiveClustering(encoder_dims=[500, 200, 50], n_kernels=len(categories), kernel_size=10)\n",
    "    model_.train()\n",
    "\n",
    "\n",
    "    ds = Dataset(X, y)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "    n_epoch = 4\n",
    "    optimizer = None\n",
    "\n",
    "    for i in range(n_epoch):\n",
    "        iteration_losses = []\n",
    "        \n",
    "        for x, labels_ in dl:\n",
    "            model_.zero_grad()\n",
    "            _ = model_(x, labels_)\n",
    "            if optimizer is None:\n",
    "                optimizer = torch.optim.Adam(model_.parameters(), lr=lr)\n",
    "\n",
    "            loss = model_.loss()\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            iteration_losses.append(loss_value)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        dic_loss[f\"entrainement_{j}\"] += iteration_losses\n",
    "        print(f\"Iteration {i} | Loss {np.mean(iteration_losses)}\")\n",
    "\n",
    "    model_.eval()\n",
    "    return model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Here we define some global variables to locate our datasets and point to our results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './'\n",
    "\n",
    "dataset_path_train_1 = root_path + 'datasets/datasets_train_test/traindata1.csv'\n",
    "dataset_path_test_1 = root_path + 'datasets/datasets_train_test/testdata1.csv'\n",
    "\n",
    "dataset_path_train_2 = root_path + 'datasets/datasets_train_test/traindata2.csv'\n",
    "dataset_path_test_2 = root_path + 'datasets/datasets_train_test/testdata2.csv'\n",
    "\n",
    "dataset_path_train_3 = root_path + 'datasets/datasets_train_test/traindata3.csv'\n",
    "dataset_path_test_3 = root_path + 'datasets/datasets_train_test/testdata3.csv'\n",
    "\n",
    "\n",
    "results_path = root_path + f'results_ekip/'\n",
    "if not os.path.exists(results_path):\n",
    "    os.mkdir(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_1 = results_path + 'trained_model_1.pkl'\n",
    "model_path_2 = results_path + 'trained_model_2.pkl'\n",
    "model_path_3 = results_path + 'trained_model_3.pkl'\n",
    "\n",
    "separator = \"-\"*50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "### Loading dataset\n",
    "\n",
    "We load the preprocessed dataset as a pandas data frame and show the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Loading dataset...\n",
      "Done loading dataset \n"
     ]
    }
   ],
   "source": [
    "print(separator)\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "train_df_1, test_df_1 = load_dataset(dataset_path_train_1, dataset_path_test_1)\n",
    "train_df_2, test_df_2 = load_dataset(dataset_path_train_2, dataset_path_test_2)\n",
    "train_df_3, test_df_3 = load_dataset(dataset_path_train_3, dataset_path_test_3)\n",
    "\n",
    "print(\"Done loading dataset \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>root_shell</th>\n",
       "      <th>su_attempted</th>\n",
       "      <th>num_root</th>\n",
       "      <th>num_file_creations</th>\n",
       "      <th>num_shells</th>\n",
       "      <th>num_access_files</th>\n",
       "      <th>num_outbound_cmds</th>\n",
       "      <th>is_host_login</th>\n",
       "      <th>is_guest_login</th>\n",
       "      <th>count</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>serror_rate</th>\n",
       "      <th>srv_serror_rate</th>\n",
       "      <th>rerror_rate</th>\n",
       "      <th>srv_rerror_rate</th>\n",
       "      <th>same_srv_rate</th>\n",
       "      <th>diff_srv_rate</th>\n",
       "      <th>srv_diff_host_rate</th>\n",
       "      <th>dst_host_count</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>Label.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>5450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>5450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>1228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "      <td>9020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "      <td>45076</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  protocol_type  service  flag  src_bytes  dst_bytes  land  \\\n",
       "0         0              0        0     0        181       5450     0   \n",
       "1         0              0        0     0        181       5450     0   \n",
       "2         0              0        0     0        236       1228     0   \n",
       "3         0              0        0     0        185       9020     0   \n",
       "4         0              0        0     0        199      45076     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  num_failed_logins  logged_in  num_compromised  \\\n",
       "0               0       0    0                  0          1                0   \n",
       "1               0       0    0                  0          1                0   \n",
       "2               0       0    0                  0          1                0   \n",
       "3               0       0    0                  0          1                0   \n",
       "4               0       0    0                  0          1                0   \n",
       "\n",
       "   root_shell  su_attempted  num_root  num_file_creations  num_shells  \\\n",
       "0           0             0         0                   0           0   \n",
       "1           0             0         0                   0           0   \n",
       "2           0             0         0                   0           0   \n",
       "3           0             0         0                   0           0   \n",
       "4           0             0         0                   0           0   \n",
       "\n",
       "   num_access_files  num_outbound_cmds  is_host_login  is_guest_login  count  \\\n",
       "0                 0                  0              0               0      8   \n",
       "1                 0                  0              0               0      2   \n",
       "2                 0                  0              0               0      3   \n",
       "3                 0                  0              0               0      1   \n",
       "4                 0                  0              0               0      2   \n",
       "\n",
       "   srv_count  serror_rate  srv_serror_rate  rerror_rate  srv_rerror_rate  \\\n",
       "0          8          0.0              0.0          0.0              0.0   \n",
       "1          2          0.0              0.0          0.0              0.0   \n",
       "2          3          0.0              0.0          0.0              0.0   \n",
       "3          1          0.0              0.0          0.0              0.0   \n",
       "4          2          0.0              0.0          0.0              0.0   \n",
       "\n",
       "   same_srv_rate  diff_srv_rate  srv_diff_host_rate  dst_host_count  \\\n",
       "0            1.0            0.0                 0.0               9   \n",
       "1            1.0            0.0                 0.0              13   \n",
       "2            1.0            0.0                 0.0              14   \n",
       "3            1.0            0.0                 0.0              20   \n",
       "4            1.0            0.0                 0.0              33   \n",
       "\n",
       "   dst_host_srv_count  dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                   9                     1.0                     0.0   \n",
       "1                  13                     1.0                     0.0   \n",
       "2                  14                     1.0                     0.0   \n",
       "3                  20                     1.0                     0.0   \n",
       "4                  33                     1.0                     0.0   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                         0.11                          0.0   \n",
       "1                         0.08                          0.0   \n",
       "2                         0.07                          0.0   \n",
       "3                         0.05                          0.0   \n",
       "4                         0.03                          0.0   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                   0.0                       0.0                   0.0   \n",
       "1                   0.0                       0.0                   0.0   \n",
       "2                   0.0                       0.0                   0.0   \n",
       "3                   0.0                       0.0                   0.0   \n",
       "4                   0.0                       0.0                   0.0   \n",
       "\n",
       "   dst_host_srv_rerror_rate  Label.  \n",
       "0                       0.0  Normal  \n",
       "1                       0.0  Normal  \n",
       "2                       0.0  Normal  \n",
       "3                       0.0  Normal  \n",
       "4                       0.0  Normal  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_1.head()\n",
    "test_df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "We prepare the features to be in the correct format for our Adaptive Clustering network and the Random Forest Classifier.\n",
    "\n",
    "The labels are separated from the training features and the order of the categories are kept track of in order to use them in the same order when evaluating the model on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating categories...\n",
      "Done creating categories...\n",
      "Creating categories...\n",
      "Done creating categories...\n",
      "Creating categories...\n",
      "Done creating categories...\n",
      "catégories de trafic traitées: ['Attack', 'Normal']\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(model_path_1 + \".categories\"):\n",
    "    print(separator)\n",
    "    print(\"Loading categories...\")\n",
    "    with open(model_path_1 + \".categories\", \"r\") as f:\n",
    "        categories = json.load(f)\n",
    "        print(f\"categories loaded: {model_path_1}.categories\")\n",
    "    print(\"Done loading categories\")\n",
    "else:\n",
    "    print(\"Creating categories...\")\n",
    "    categories = list(set(pd.factorize(train_df_1['Label.'])[1].values))\n",
    "    print(\"Done creating categories...\")\n",
    "\n",
    "if os.path.exists(model_path_2 + \".categories\"):\n",
    "    print(separator)\n",
    "    print(\"Loading categories...\")\n",
    "    with open(model_path_2 + \".categories\", \"r\") as f:\n",
    "        categories = json.load(f)\n",
    "        print(f\"categories loaded: {model_path_2}.categories\")\n",
    "    print(\"Done loading categories\")\n",
    "else:\n",
    "    print(\"Creating categories...\")\n",
    "    categories = list(set(pd.factorize(train_df_2['Label.'])[1].values))\n",
    "    print(\"Done creating categories...\")\n",
    "\n",
    "if os.path.exists(model_path_3 + \".categories\"):\n",
    "    print(separator)\n",
    "    print(\"Loading categories...\")\n",
    "    with open(model_path_3 + \".categories\", \"r\") as f:\n",
    "        categories = json.load(f)\n",
    "        print(f\"categories loaded: {model_path_3}.categories\")\n",
    "    print(\"Done loading categories\")\n",
    "else:\n",
    "    print(\"Creating categories...\")\n",
    "    categories = list(set(pd.factorize(train_df_3['Label.'])[1].values))\n",
    "    print(\"Done creating categories...\")\n",
    "\n",
    "\n",
    "print(\"catégories de trafic traitées:\",categories)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_df1_ = train_df_1.drop(['Label.'], axis=1)\n",
    "train_df2_ = train_df_2.drop(['Label.'], axis=1)\n",
    "train_df3_ = train_df_3.drop(['Label.'], axis=1)\n",
    "\n",
    "X1 = train_df1_.values.tolist()\n",
    "y1 = train_df_1['Label.'].values.tolist()\n",
    "\n",
    "X2 = train_df2_.values.tolist()\n",
    "y2 = train_df_2['Label.'].values.tolist()\n",
    "\n",
    "X3 = train_df3_.values.tolist()\n",
    "y3 = train_df_3['Label.'].values.tolist()\n",
    "\n",
    "X1 = torch.FloatTensor(np.array(X1))\n",
    "X2 = torch.FloatTensor(np.array(X2))\n",
    "X3 = torch.FloatTensor(np.array(X3))\n",
    "\n",
    "for i, l in enumerate(y1):\n",
    "    y1[i] = categories.index(l)\n",
    "\n",
    "for i, l in enumerate(y2):\n",
    "    y2[i] = categories.index(l)\n",
    "\n",
    "for i, l in enumerate(y3):\n",
    "    y3[i] = categories.index(l)\n",
    "\n",
    "y1 = torch.LongTensor(np.array(y1))\n",
    "y2 = torch.LongTensor(np.array(y2))\n",
    "y3 = torch.LongTensor(np.array(y3))\n",
    "\n",
    "\n",
    "# df.drop(['Label.'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "We train our model until we achieve an acceptable loss and export an extended dataset with the cluster centers obtained from the Adaptive Clustering network. This would allow us to not have to retrain our network for every single execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 17:56:11] Invalid gpu_ids format. Expected a string or a list of ints.\n",
      "[codecarbon INFO @ 17:56:11] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 17:56:11] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 17:56:11] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 17:56:11] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 17:56:11] No CPU tracking mode found. Falling back on CPU constant mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 17:56:12] CPU Model on constant consumption mode: AMD Ryzen 9 5900HX with Radeon Graphics\n",
      "[codecarbon INFO @ 17:56:12] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 17:56:12]   Platform system: Windows-10-10.0.22631-SP0\n",
      "[codecarbon INFO @ 17:56:12]   Python version: 3.11.9\n",
      "[codecarbon INFO @ 17:56:12]   CodeCarbon version: 2.4.1\n",
      "[codecarbon INFO @ 17:56:12]   Available RAM : 15.422 GB\n",
      "[codecarbon INFO @ 17:56:12]   CPU count: 16\n",
      "[codecarbon INFO @ 17:56:12]   CPU model: AMD Ryzen 9 5900HX with Radeon Graphics\n",
      "[codecarbon INFO @ 17:56:12]   GPU count: 1\n",
      "[codecarbon INFO @ 17:56:12]   GPU model: 1 x NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 17:56:28] Energy consumed for RAM : 0.000024 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 17:56:28] Energy consumed for all GPUs : 0.000061 kWh. Total GPU Power : 14.59456774019484 W\n",
      "[codecarbon INFO @ 17:56:28] Energy consumed for all CPUs : 0.000094 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:56:28] 0.000179 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:56:43] Energy consumed for RAM : 0.000048 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 17:56:43] Energy consumed for all GPUs : 0.000119 kWh. Total GPU Power : 13.854010570211893 W\n",
      "[codecarbon INFO @ 17:56:43] Energy consumed for all CPUs : 0.000188 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:56:43] 0.000354 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:56:58] Energy consumed for RAM : 0.000072 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 17:56:58] Energy consumed for all GPUs : 0.000177 kWh. Total GPU Power : 13.939390964565098 W\n",
      "[codecarbon INFO @ 17:56:58] Energy consumed for all CPUs : 0.000281 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:56:58] 0.000530 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:57:13] Energy consumed for RAM : 0.000096 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 17:57:13] Energy consumed for all GPUs : 0.000235 kWh. Total GPU Power : 14.025268925151506 W\n",
      "[codecarbon INFO @ 17:57:13] Energy consumed for all CPUs : 0.000375 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:57:13] 0.000707 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:57:28] Energy consumed for RAM : 0.000120 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 17:57:28] Energy consumed for all GPUs : 0.000292 kWh. Total GPU Power : 13.714836388839958 W\n",
      "[codecarbon INFO @ 17:57:28] Energy consumed for all CPUs : 0.000469 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:57:28] 0.000882 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:57:43] Energy consumed for RAM : 0.000145 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 17:57:43] Energy consumed for all GPUs : 0.000352 kWh. Total GPU Power : 14.425975007785778 W\n",
      "[codecarbon INFO @ 17:57:43] Energy consumed for all CPUs : 0.000563 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:57:43] 0.001060 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:57:58] Energy consumed for RAM : 0.000169 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 17:57:58] Energy consumed for all GPUs : 0.000408 kWh. Total GPU Power : 13.398717846735353 W\n",
      "[codecarbon INFO @ 17:57:58] Energy consumed for all CPUs : 0.000656 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:57:58] 0.001233 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:58:13] Energy consumed for RAM : 0.000193 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 17:58:13] Energy consumed for all GPUs : 0.000470 kWh. Total GPU Power : 14.815123849446982 W\n",
      "[codecarbon INFO @ 17:58:13] Energy consumed for all CPUs : 0.000750 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:58:13] 0.001413 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:58:28] Energy consumed for RAM : 0.000217 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 17:58:28] Energy consumed for all GPUs : 0.000529 kWh. Total GPU Power : 14.224976641614429 W\n",
      "[codecarbon INFO @ 17:58:28] Energy consumed for all CPUs : 0.000844 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:58:28] 0.001590 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:58:43] Energy consumed for RAM : 0.000241 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 17:58:43] Energy consumed for all GPUs : 0.000588 kWh. Total GPU Power : 14.211784631359821 W\n",
      "[codecarbon INFO @ 17:58:43] Energy consumed for all CPUs : 0.000938 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:58:43] 0.001767 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:58:58] Energy consumed for RAM : 0.000265 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 17:58:58] Energy consumed for all GPUs : 0.000648 kWh. Total GPU Power : 14.219064846841396 W\n",
      "[codecarbon INFO @ 17:58:58] Energy consumed for all CPUs : 0.001032 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:58:58] 0.001944 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:59:13] Energy consumed for RAM : 0.000289 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 17:59:13] Energy consumed for all GPUs : 0.000707 kWh. Total GPU Power : 14.168967855765947 W\n",
      "[codecarbon INFO @ 17:59:13] Energy consumed for all CPUs : 0.001125 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:59:13] 0.002121 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:59:28] Energy consumed for RAM : 0.000313 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 17:59:28] Energy consumed for all GPUs : 0.000768 kWh. Total GPU Power : 14.7377137304137 W\n",
      "[codecarbon INFO @ 17:59:28] Energy consumed for all CPUs : 0.001219 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:59:28] 0.002301 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:59:43] Energy consumed for RAM : 0.000337 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 17:59:43] Energy consumed for all GPUs : 0.000827 kWh. Total GPU Power : 14.237361575374534 W\n",
      "[codecarbon INFO @ 17:59:43] Energy consumed for all CPUs : 0.001313 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:59:43] 0.002478 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:59:58] Energy consumed for RAM : 0.000361 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 17:59:58] Energy consumed for all GPUs : 0.000885 kWh. Total GPU Power : 13.861292855081054 W\n",
      "[codecarbon INFO @ 17:59:58] Energy consumed for all CPUs : 0.001407 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:59:58] 0.002653 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:00:13] Energy consumed for RAM : 0.000385 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:00:13] Energy consumed for all GPUs : 0.000944 kWh. Total GPU Power : 14.099315979065864 W\n",
      "[codecarbon INFO @ 18:00:13] Energy consumed for all CPUs : 0.001501 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:00:13] 0.002830 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:00:28] Energy consumed for RAM : 0.000410 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:00:28] Energy consumed for all GPUs : 0.001004 kWh. Total GPU Power : 14.372372288551707 W\n",
      "[codecarbon INFO @ 18:00:28] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:00:28] 0.003008 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:00:43] Energy consumed for RAM : 0.000434 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:00:43] Energy consumed for all GPUs : 0.001057 kWh. Total GPU Power : 12.637111490151668 W\n",
      "[codecarbon INFO @ 18:00:43] Energy consumed for all CPUs : 0.001688 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:00:43] 0.003178 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:00:58] Energy consumed for RAM : 0.000458 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:00:58] Energy consumed for all GPUs : 0.001118 kWh. Total GPU Power : 14.795439960961607 W\n",
      "[codecarbon INFO @ 18:00:58] Energy consumed for all CPUs : 0.001782 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:00:58] 0.003358 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:01:13] Energy consumed for RAM : 0.000482 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:01:13] Energy consumed for all GPUs : 0.001179 kWh. Total GPU Power : 14.583179556564897 W\n",
      "[codecarbon INFO @ 18:01:13] Energy consumed for all CPUs : 0.001876 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:01:13] 0.003536 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:01:28] Energy consumed for RAM : 0.000506 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:01:28] Energy consumed for all GPUs : 0.001237 kWh. Total GPU Power : 14.001015564958985 W\n",
      "[codecarbon INFO @ 18:01:28] Energy consumed for all CPUs : 0.001969 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:01:28] 0.003713 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:01:43] Energy consumed for RAM : 0.000530 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:01:43] Energy consumed for all GPUs : 0.001296 kWh. Total GPU Power : 14.119101965962606 W\n",
      "[codecarbon INFO @ 18:01:43] Energy consumed for all CPUs : 0.002063 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:01:43] 0.003889 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:01:58] Energy consumed for RAM : 0.000554 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:01:58] Energy consumed for all GPUs : 0.001365 kWh. Total GPU Power : 16.562928109819822 W\n",
      "[codecarbon INFO @ 18:01:58] Energy consumed for all CPUs : 0.002157 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:01:58] 0.004076 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:02:13] Energy consumed for RAM : 0.000578 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:02:13] Energy consumed for all GPUs : 0.001413 kWh. Total GPU Power : 11.58239078611882 W\n",
      "[codecarbon INFO @ 18:02:13] Energy consumed for all CPUs : 0.002251 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:02:13] 0.004242 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:02:28] Energy consumed for RAM : 0.000602 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:02:28] Energy consumed for all GPUs : 0.001463 kWh. Total GPU Power : 11.96806101612622 W\n",
      "[codecarbon INFO @ 18:02:28] Energy consumed for all CPUs : 0.002345 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:02:28] 0.004410 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:02:43] Energy consumed for RAM : 0.000626 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:02:43] Energy consumed for all GPUs : 0.001519 kWh. Total GPU Power : 13.31958971310386 W\n",
      "[codecarbon INFO @ 18:02:43] Energy consumed for all CPUs : 0.002438 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:02:43] 0.004584 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:02:58] Energy consumed for RAM : 0.000650 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:02:58] Energy consumed for all GPUs : 0.001573 kWh. Total GPU Power : 13.016217004451535 W\n",
      "[codecarbon INFO @ 18:02:58] Energy consumed for all CPUs : 0.002532 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:02:58] 0.004756 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:03:13] Energy consumed for RAM : 0.000675 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:03:13] Energy consumed for all GPUs : 0.001638 kWh. Total GPU Power : 15.59125546403154 W\n",
      "[codecarbon INFO @ 18:03:13] Energy consumed for all CPUs : 0.002626 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:03:13] 0.004939 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:03:28] Energy consumed for RAM : 0.000699 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:03:28] Energy consumed for all GPUs : 0.001694 kWh. Total GPU Power : 13.531333574554418 W\n",
      "[codecarbon INFO @ 18:03:28] Energy consumed for all CPUs : 0.002720 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:03:28] 0.005113 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:03:43] Energy consumed for RAM : 0.000723 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:03:43] Energy consumed for all GPUs : 0.001754 kWh. Total GPU Power : 14.293691433091771 W\n",
      "[codecarbon INFO @ 18:03:43] Energy consumed for all CPUs : 0.002814 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:03:43] 0.005290 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:03:58] Energy consumed for RAM : 0.000747 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:03:58] Energy consumed for all GPUs : 0.001812 kWh. Total GPU Power : 13.805911972186632 W\n",
      "[codecarbon INFO @ 18:03:58] Energy consumed for all CPUs : 0.002907 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:03:58] 0.005466 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:04:13] Energy consumed for RAM : 0.000771 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:04:13] Energy consumed for all GPUs : 0.001870 kWh. Total GPU Power : 14.084220511275737 W\n",
      "[codecarbon INFO @ 18:04:13] Energy consumed for all CPUs : 0.003001 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:04:13] 0.005642 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:04:28] Energy consumed for RAM : 0.000795 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:04:28] Energy consumed for all GPUs : 0.001929 kWh. Total GPU Power : 14.085964383934916 W\n",
      "[codecarbon INFO @ 18:04:28] Energy consumed for all CPUs : 0.003095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:04:28] 0.005819 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:04:43] Energy consumed for RAM : 0.000819 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:04:43] Energy consumed for all GPUs : 0.001990 kWh. Total GPU Power : 14.766828310460946 W\n",
      "[codecarbon INFO @ 18:04:43] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:04:43] 0.005998 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:04:58] Energy consumed for RAM : 0.000843 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:04:58] Energy consumed for all GPUs : 0.002049 kWh. Total GPU Power : 13.971240116545504 W\n",
      "[codecarbon INFO @ 18:04:58] Energy consumed for all CPUs : 0.003283 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:04:58] 0.006174 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:05:13] Energy consumed for RAM : 0.000867 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:05:13] Energy consumed for all GPUs : 0.002107 kWh. Total GPU Power : 13.92836889564384 W\n",
      "[codecarbon INFO @ 18:05:13] Energy consumed for all CPUs : 0.003376 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:05:13] 0.006350 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:05:28] Energy consumed for RAM : 0.000892 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:05:28] Energy consumed for all GPUs : 0.002166 kWh. Total GPU Power : 14.07619633246093 W\n",
      "[codecarbon INFO @ 18:05:28] Energy consumed for all CPUs : 0.003471 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:05:28] 0.006528 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:05:43] Energy consumed for RAM : 0.000916 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:05:43] Energy consumed for all GPUs : 0.002226 kWh. Total GPU Power : 14.423701087160456 W\n",
      "[codecarbon INFO @ 18:05:43] Energy consumed for all CPUs : 0.003565 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:05:43] 0.006707 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:05:58] Energy consumed for RAM : 0.000940 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:05:58] Energy consumed for all GPUs : 0.002285 kWh. Total GPU Power : 14.067933016915033 W\n",
      "[codecarbon INFO @ 18:05:58] Energy consumed for all CPUs : 0.003659 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:05:58] 0.006883 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:06:13] Energy consumed for RAM : 0.000964 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:06:13] Energy consumed for all GPUs : 0.002345 kWh. Total GPU Power : 14.410463838910305 W\n",
      "[codecarbon INFO @ 18:06:13] Energy consumed for all CPUs : 0.003752 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:06:13] 0.007061 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:06:28] Energy consumed for RAM : 0.000988 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:06:28] Energy consumed for all GPUs : 0.002402 kWh. Total GPU Power : 13.690020205098786 W\n",
      "[codecarbon INFO @ 18:06:28] Energy consumed for all CPUs : 0.003846 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:06:28] 0.007236 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 | Loss 0.1421681706353369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:06:43] Energy consumed for RAM : 0.001012 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:06:43] Energy consumed for all GPUs : 0.002461 kWh. Total GPU Power : 14.256475117342243 W\n",
      "[codecarbon INFO @ 18:06:43] Energy consumed for all CPUs : 0.003940 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:06:43] 0.007413 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:06:58] Energy consumed for RAM : 0.001036 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:06:59] Energy consumed for all GPUs : 0.002512 kWh. Total GPU Power : 12.28454316541029 W\n",
      "[codecarbon INFO @ 18:06:59] Energy consumed for all CPUs : 0.004037 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:06:59] 0.007585 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:07:13] Energy consumed for RAM : 0.001059 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:07:14] Energy consumed for all GPUs : 0.002547 kWh. Total GPU Power : 8.668833776346016 W\n",
      "[codecarbon INFO @ 18:07:14] Energy consumed for all CPUs : 0.004130 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:07:14] 0.007737 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:07:28] Energy consumed for RAM : 0.001083 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:07:29] Energy consumed for all GPUs : 0.002580 kWh. Total GPU Power : 8.049329015040907 W\n",
      "[codecarbon INFO @ 18:07:29] Energy consumed for all CPUs : 0.004224 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:07:29] 0.007887 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:07:43] Energy consumed for RAM : 0.001106 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:07:43] Energy consumed for all GPUs : 0.002628 kWh. Total GPU Power : 11.947406916365084 W\n",
      "[codecarbon INFO @ 18:07:43] Energy consumed for all CPUs : 0.004315 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:07:43] 0.008049 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:07:58] Energy consumed for RAM : 0.001130 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:07:58] Energy consumed for all GPUs : 0.002676 kWh. Total GPU Power : 11.51664956067859 W\n",
      "[codecarbon INFO @ 18:07:58] Energy consumed for all CPUs : 0.004409 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:07:58] 0.008215 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:13] Energy consumed for RAM : 0.001154 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:08:13] Energy consumed for all GPUs : 0.002725 kWh. Total GPU Power : 11.674800584396614 W\n",
      "[codecarbon INFO @ 18:08:13] Energy consumed for all CPUs : 0.004503 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:08:13] 0.008382 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:28] Energy consumed for RAM : 0.001178 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:08:28] Energy consumed for all GPUs : 0.002773 kWh. Total GPU Power : 11.54764444946501 W\n",
      "[codecarbon INFO @ 18:08:28] Energy consumed for all CPUs : 0.004596 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:08:28] 0.008548 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:43] Energy consumed for RAM : 0.001203 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:08:43] Energy consumed for all GPUs : 0.002820 kWh. Total GPU Power : 11.426965092849388 W\n",
      "[codecarbon INFO @ 18:08:43] Energy consumed for all CPUs : 0.004690 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:08:43] 0.008713 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:58] Energy consumed for RAM : 0.001227 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:08:58] Energy consumed for all GPUs : 0.002868 kWh. Total GPU Power : 11.460105346481855 W\n",
      "[codecarbon INFO @ 18:08:58] Energy consumed for all CPUs : 0.004784 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:08:58] 0.008879 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:13] Energy consumed for RAM : 0.001251 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:09:13] Energy consumed for all GPUs : 0.002920 kWh. Total GPU Power : 12.374824659084934 W\n",
      "[codecarbon INFO @ 18:09:13] Energy consumed for all CPUs : 0.004878 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:09:13] 0.009048 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:28] Energy consumed for RAM : 0.001275 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:09:29] Energy consumed for all GPUs : 0.002973 kWh. Total GPU Power : 12.846896585026533 W\n",
      "[codecarbon INFO @ 18:09:29] Energy consumed for all CPUs : 0.004972 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:09:29] 0.009220 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:43] Energy consumed for RAM : 0.001299 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:09:44] Energy consumed for all GPUs : 0.003026 kWh. Total GPU Power : 12.715731566875954 W\n",
      "[codecarbon INFO @ 18:09:44] Energy consumed for all CPUs : 0.005065 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:09:44] 0.009391 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:58] Energy consumed for RAM : 0.001323 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:09:59] Energy consumed for all GPUs : 0.003079 kWh. Total GPU Power : 12.618166280412614 W\n",
      "[codecarbon INFO @ 18:09:59] Energy consumed for all CPUs : 0.005159 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:09:59] 0.009561 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:14] Energy consumed for RAM : 0.001347 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:10:14] Energy consumed for all GPUs : 0.003133 kWh. Total GPU Power : 12.910269743781678 W\n",
      "[codecarbon INFO @ 18:10:14] Energy consumed for all CPUs : 0.005253 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:10:14] 0.009733 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:29] Energy consumed for RAM : 0.001371 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:10:29] Energy consumed for all GPUs : 0.003191 kWh. Total GPU Power : 13.991930259316367 W\n",
      "[codecarbon INFO @ 18:10:29] Energy consumed for all CPUs : 0.005347 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:10:29] 0.009909 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:44] Energy consumed for RAM : 0.001395 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:10:44] Energy consumed for all GPUs : 0.003249 kWh. Total GPU Power : 13.981901597409184 W\n",
      "[codecarbon INFO @ 18:10:44] Energy consumed for all CPUs : 0.005441 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:10:44] 0.010085 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:59] Energy consumed for RAM : 0.001419 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:10:59] Energy consumed for all GPUs : 0.003303 kWh. Total GPU Power : 12.893102475479997 W\n",
      "[codecarbon INFO @ 18:10:59] Energy consumed for all CPUs : 0.005534 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:10:59] 0.010257 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:14] Energy consumed for RAM : 0.001443 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:11:14] Energy consumed for all GPUs : 0.003351 kWh. Total GPU Power : 11.614493279786405 W\n",
      "[codecarbon INFO @ 18:11:14] Energy consumed for all CPUs : 0.005628 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:11:14] 0.010423 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:29] Energy consumed for RAM : 0.001468 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:11:29] Energy consumed for all GPUs : 0.003399 kWh. Total GPU Power : 11.452006987888959 W\n",
      "[codecarbon INFO @ 18:11:29] Energy consumed for all CPUs : 0.005722 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:11:29] 0.010589 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:44] Energy consumed for RAM : 0.001492 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:11:44] Energy consumed for all GPUs : 0.003447 kWh. Total GPU Power : 11.524813527613864 W\n",
      "[codecarbon INFO @ 18:11:44] Energy consumed for all CPUs : 0.005816 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:11:44] 0.010755 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:59] Energy consumed for RAM : 0.001516 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:11:59] Energy consumed for all GPUs : 0.003495 kWh. Total GPU Power : 11.429857248563692 W\n",
      "[codecarbon INFO @ 18:11:59] Energy consumed for all CPUs : 0.005909 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:11:59] 0.010920 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:12:14] Energy consumed for RAM : 0.001540 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:12:14] Energy consumed for all GPUs : 0.003542 kWh. Total GPU Power : 11.413111800425092 W\n",
      "[codecarbon INFO @ 18:12:14] Energy consumed for all CPUs : 0.006003 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:12:14] 0.011086 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:12:29] Energy consumed for RAM : 0.001564 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:12:29] Energy consumed for all GPUs : 0.003590 kWh. Total GPU Power : 11.457409880598904 W\n",
      "[codecarbon INFO @ 18:12:29] Energy consumed for all CPUs : 0.006097 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:12:29] 0.011251 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:12:44] Energy consumed for RAM : 0.001588 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:12:44] Energy consumed for all GPUs : 0.003639 kWh. Total GPU Power : 11.639221003568704 W\n",
      "[codecarbon INFO @ 18:12:44] Energy consumed for all CPUs : 0.006191 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:12:44] 0.011418 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:12:59] Energy consumed for RAM : 0.001612 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:12:59] Energy consumed for all GPUs : 0.003693 kWh. Total GPU Power : 13.02992640319404 W\n",
      "[codecarbon INFO @ 18:12:59] Energy consumed for all CPUs : 0.006285 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:12:59] 0.011590 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:13:14] Energy consumed for RAM : 0.001636 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:13:14] Energy consumed for all GPUs : 0.003745 kWh. Total GPU Power : 12.47105953233818 W\n",
      "[codecarbon INFO @ 18:13:14] Energy consumed for all CPUs : 0.006379 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:13:14] 0.011760 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:13:29] Energy consumed for RAM : 0.001660 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:13:29] Energy consumed for all GPUs : 0.003803 kWh. Total GPU Power : 13.857810938294861 W\n",
      "[codecarbon INFO @ 18:13:29] Energy consumed for all CPUs : 0.006472 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:13:29] 0.011935 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:13:44] Energy consumed for RAM : 0.001684 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:13:44] Energy consumed for all GPUs : 0.003859 kWh. Total GPU Power : 13.447686250721233 W\n",
      "[codecarbon INFO @ 18:13:44] Energy consumed for all CPUs : 0.006566 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:13:44] 0.012109 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:13:59] Energy consumed for RAM : 0.001708 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:13:59] Energy consumed for all GPUs : 0.003915 kWh. Total GPU Power : 13.620150985393442 W\n",
      "[codecarbon INFO @ 18:13:59] Energy consumed for all CPUs : 0.006660 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:13:59] 0.012284 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:14:14] Energy consumed for RAM : 0.001733 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:14:14] Energy consumed for all GPUs : 0.003972 kWh. Total GPU Power : 13.65306647008798 W\n",
      "[codecarbon INFO @ 18:14:14] Energy consumed for all CPUs : 0.006754 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:14:14] 0.012459 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:14:29] Energy consumed for RAM : 0.001757 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:14:29] Energy consumed for all GPUs : 0.004029 kWh. Total GPU Power : 13.560854990300058 W\n",
      "[codecarbon INFO @ 18:14:29] Energy consumed for all CPUs : 0.006848 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:14:29] 0.012634 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:14:44] Energy consumed for RAM : 0.001781 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:14:44] Energy consumed for all GPUs : 0.004085 kWh. Total GPU Power : 13.445233091969488 W\n",
      "[codecarbon INFO @ 18:14:44] Energy consumed for all CPUs : 0.006942 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:14:44] 0.012808 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:14:59] Energy consumed for RAM : 0.001805 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:14:59] Energy consumed for all GPUs : 0.004141 kWh. Total GPU Power : 13.385567179231721 W\n",
      "[codecarbon INFO @ 18:14:59] Energy consumed for all CPUs : 0.007036 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:14:59] 0.012982 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:15:14] Energy consumed for RAM : 0.001829 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:15:14] Energy consumed for all GPUs : 0.004198 kWh. Total GPU Power : 13.661987109302983 W\n",
      "[codecarbon INFO @ 18:15:14] Energy consumed for all CPUs : 0.007130 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:15:14] 0.013157 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:15:29] Energy consumed for RAM : 0.001853 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:15:29] Energy consumed for all GPUs : 0.004252 kWh. Total GPU Power : 12.956543963886933 W\n",
      "[codecarbon INFO @ 18:15:29] Energy consumed for all CPUs : 0.007223 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:15:29] 0.013329 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:15:44] Energy consumed for RAM : 0.001877 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:15:44] Energy consumed for all GPUs : 0.004308 kWh. Total GPU Power : 13.484350496835035 W\n",
      "[codecarbon INFO @ 18:15:44] Energy consumed for all CPUs : 0.007317 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:15:44] 0.013503 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:15:59] Energy consumed for RAM : 0.001901 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:15:59] Energy consumed for all GPUs : 0.004363 kWh. Total GPU Power : 13.125963243593414 W\n",
      "[codecarbon INFO @ 18:15:59] Energy consumed for all CPUs : 0.007411 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:15:59] 0.013675 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:16:14] Energy consumed for RAM : 0.001926 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:16:14] Energy consumed for all GPUs : 0.004419 kWh. Total GPU Power : 13.40651900004495 W\n",
      "[codecarbon INFO @ 18:16:14] Energy consumed for all CPUs : 0.007505 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:16:14] 0.013849 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:16:29] Energy consumed for RAM : 0.001950 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:16:29] Energy consumed for all GPUs : 0.004475 kWh. Total GPU Power : 13.465871014538795 W\n",
      "[codecarbon INFO @ 18:16:29] Energy consumed for all CPUs : 0.007598 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:16:29] 0.014023 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:16:44] Energy consumed for RAM : 0.001974 kWh. RAM Power : 5.783294677734375 W\n",
      "[codecarbon INFO @ 18:16:44] Energy consumed for all GPUs : 0.004528 kWh. Total GPU Power : 12.627345399482792 W\n",
      "[codecarbon INFO @ 18:16:44] Energy consumed for all CPUs : 0.007692 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:16:44] 0.014193 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 | Loss 0.010614396262463974\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(model_path_1):\n",
    "    global dic_loss_1\n",
    "    dic_loss_1 = {}\n",
    "    dico_temps_1 = {}\n",
    "    print(\"Training model...\")\n",
    "\n",
    "    debut = time.time()\n",
    "    date = datetime.now()\n",
    "    model_1 = train(X1, y1, 1,dic_loss_1)\n",
    "    durée = time.time() - debut\n",
    "    dico_temps_1[\"entrainement_1\"] = [date,durée]\n",
    "\n",
    "    # Sauvegarde du dictionnaire_loss dans un fichier\n",
    "    with open(results_path+ \"jeu_de_donnees/\" + \"dic_loss_1.pkl\", 'wb') as file:\n",
    "        pkl.dump(dic_loss_1, file)\n",
    "\n",
    "    # Sauvegarde du dictionnaire_temps dans un fichier\n",
    "    with open(results_path+ \"jeu_de_donnees/\" + \"dico_temps_1.pkl\", 'wb') as file:\n",
    "        pkl.dump(dico_temps_1, file)\n",
    "\n",
    "    print(\"Done training model\")\n",
    "\n",
    "if not os.path.exists(model_path_2):\n",
    "    global dic_loss_2\n",
    "    dic_loss_2 = {}\n",
    "    dico_temps_2 = {}\n",
    "    print(\"Training model...\")\n",
    "\n",
    "    debut = time.time()\n",
    "    date = datetime.now()\n",
    "    model_2 = train(X2, y2, 2,dic_loss_2)\n",
    "    durée = time.time() - debut\n",
    "    dico_temps_2[\"entrainement_2\"] = [date,durée]\n",
    "\n",
    "    # Sauvegarde du dictionnaire_loss dans un fichier\n",
    "    with open(results_path+ \"jeu_de_donnees/\" + \"dic_loss_2.pkl\", 'wb') as file:\n",
    "        pkl.dump(dic_loss_2, file)\n",
    "\n",
    "    # Sauvegarde du dictionnaire_temps dans un fichier\n",
    "    with open(results_path+ \"jeu_de_donnees/\" + \"dico_temps_2.pkl\", 'wb') as file:\n",
    "        pkl.dump(dico_temps_2, file)\n",
    "        \n",
    "    print(\"Done training model\")\n",
    "\n",
    "if not os.path.exists(model_path_3):\n",
    "    global dic_loss_3\n",
    "    dic_loss_3 = {}\n",
    "    dico_temps_3 = {}\n",
    "    print(\"Training model...\")\n",
    "\n",
    "    debut = time.time()\n",
    "    date = datetime.now()\n",
    "    model_3 = train(X3, y3, 3,dic_loss_3)\n",
    "    durée = time.time() - debut\n",
    "    dico_temps_3[\"entrainement_3\"] = [date,durée]\n",
    "\n",
    "    # Sauvegarde du dictionnaire_loss dans un fichier\n",
    "    with open(results_path+ \"jeu_de_donnees/\" + \"dic_loss_3.pkl\", 'wb') as file:\n",
    "        pkl.dump(dic_loss_3, file)\n",
    "\n",
    "    # Sauvegarde du dictionnaire_temps dans un fichier\n",
    "    with open(results_path+ \"jeu_de_donnees/\" + \"dico_temps_3.pkl\", 'wb') as file:\n",
    "        pkl.dump(dico_temps_3, file)\n",
    "        \n",
    "    print(\"Done training model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining the extended dataset, we train our final classifier and evaluate our Intrusion Detection System."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_path_1):\n",
    "    print(\"Training Random Forest...\")\n",
    "    model_1.classifier = RandomForest(n_estimators=200)\n",
    "    model_1.classifier.fit(X1, y1)\n",
    "    print(\"Done training Random Forest\")\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    save_model(model_1, model_path_1)\n",
    "    with open(model_path_1+\".categories\", \"w\") as f:\n",
    "        json.dump(categories, f)\n",
    "        print(f\"Categories saved: {model_path_1}.categories\")\n",
    "    print(\"Done saving model\")\n",
    "\n",
    "if not os.path.exists(model_path_2):\n",
    "    print(\"Training Random Forest...\")\n",
    "    model_2.classifier = RandomForest(n_estimators=200)\n",
    "    model_2.classifier.fit(X2, y2)\n",
    "    print(\"Done training Random Forest\")\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    save_model(model_2, model_path_2)\n",
    "    with open(model_path_2+\".categories\", \"w\") as f:\n",
    "        json.dump(categories, f)\n",
    "        print(f\"Categories saved: {model_path_2}.categories\")\n",
    "    print(\"Done saving model\")\n",
    "\n",
    "if not os.path.exists(model_path_3):\n",
    "    print(\"Training Random Forest...\")\n",
    "    model_3.classifier = RandomForest(n_estimators=200)\n",
    "    model_3.classifier.fit(X3, y3)\n",
    "    print(\"Done training Random Forest\")\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    save_model(model_3, model_path_3)\n",
    "    with open(model_path_3+\".categories\", \"w\") as f:\n",
    "        json.dump(categories, f)\n",
    "        print(f\"Categories saved: {model_path_3}.categories\")\n",
    "    print(\"Done saving model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(model_path_1):\n",
    "    print(\"Loading model...\")\n",
    "    model_1 = load_model(model_path_1)\n",
    "    print(\"Done loading model\")\n",
    "\n",
    "if os.path.exists(model_path_2):\n",
    "    print(\"Loading model...\")\n",
    "    model_2 = load_model(model_path_2)\n",
    "    print(\"Done loading model\")\n",
    "\n",
    "if os.path.exists(model_path_3):\n",
    "    print(\"Loading model...\")\n",
    "    model_3 = load_model(model_path_3)\n",
    "    print(\"Done loading model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distributions \n",
    "\n",
    "Here, we print our data distributions per traffic category to verify that we have all categories of our dataset in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(separator+'MODELE 1'+separator)\n",
    "print(f\"Total data distribution: {len(test_df_1) + len(train_df_1)}\")\n",
    "total = {}\n",
    "total['total'] = len(test_df_1) + len(train_df_1)\n",
    "for cat in categories:\n",
    "    total[cat] = len(test_df_1[test_df_1['Label.'] == cat]) \\\n",
    "                   + len(train_df_1[train_df_1['Label.'] == cat])\n",
    "    print(f\"\\t{cat}: {total[cat]}\")\n",
    "\n",
    "print(f\"Training data distribution: {len(train_df_1)} / {len(train_df_1) * 100 / total['total']}%\")\n",
    "for cat in categories:\n",
    "    c = len(train_df_1[train_df_1['Label.'] == cat])\n",
    "    print(f\"\\t{cat}: {c} / {c*100 / total[cat]}%\")\n",
    "    \n",
    "print(f\"Testing data distribution: {len(test_df_1)} / {len(test_df_1) * 100 / total['total']}%\")\n",
    "for cat in categories:\n",
    "    c = len(test_df_1[test_df_1['Label.'] == cat])\n",
    "    print(f\"\\t{cat}: {c} / {c * 100 / total[cat]}%\")\n",
    "print(\"#############################################################################################################\")\n",
    "\n",
    "print(separator+'MODELE 2'+separator)\n",
    "print(f\"Total data distribution: {len(test_df_2) + len(train_df_2)}\")\n",
    "total = {}\n",
    "total['total'] = len(test_df_2) + len(train_df_2)\n",
    "for cat in categories:\n",
    "    total[cat] = len(test_df_2[test_df_2['Label.'] == cat]) \\\n",
    "                   + len(train_df_2[train_df_2['Label.'] == cat])\n",
    "    print(f\"\\t{cat}: {total[cat]}\")\n",
    "print(f\"Training data distribution: {len(train_df_2)} / {len(train_df_2) * 100 / total['total']}%\")\n",
    "for cat in categories:\n",
    "    c = len(train_df_2[train_df_2['Label.'] == cat])\n",
    "    print(f\"\\t{cat}: {c} / {c*100 / total[cat]}%\")\n",
    "print(f\"Testing data distribution: {len(test_df_2)} / {len(test_df_2) * 100 / total['total']}%\")\n",
    "for cat in categories:\n",
    "    c = len(test_df_2[test_df_2['Label.'] == cat])\n",
    "    print(f\"\\t{cat}: {c} / {c * 100 / total[cat]}%\")\n",
    "print(\"#############################################################################################################\")\n",
    "\n",
    "print(separator+'MODELE 3'+separator)\n",
    "print(f\"Total data distribution: {len(test_df_3) + len(train_df_3)}\")\n",
    "total = {}\n",
    "total['total'] = len(test_df_3) + len(train_df_3)\n",
    "for cat in categories:\n",
    "    total[cat] = len(test_df_3[test_df_3['Label.'] == cat]) \\\n",
    "                   + len(train_df_3[train_df_3['Label.'] == cat])\n",
    "    print(f\"\\t{cat}: {total[cat]}\")\n",
    "print(f\"Training data distribution: {len(train_df_3)} / {len(train_df_3) * 100 / total['total']}%\")\n",
    "for cat in categories:\n",
    "    c = len(train_df_3[train_df_3['Label.'] == cat])\n",
    "    print(f\"\\t{cat}: {c} / {c*100 / total[cat]}%\")\n",
    "print(f\"Testing data distribution: {len(test_df_3)} / {len(test_df_3) * 100 / total['total']}%\")\n",
    "for cat in categories:\n",
    "    c = len(test_df_3[test_df_3['Label.'] == cat])\n",
    "    print(f\"\\t{cat}: {c} / {c * 100 / total[cat]}%\")\n",
    "print(\"#############################################################################################################\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "\n",
    "To evaluate our model on the unseen set, we remove the labels and run our trained model on the remaining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = test_df_1.drop(['Label.'], axis=1)\n",
    "df2 = test_df_2.drop(['Label.'], axis=1)\n",
    "df3 = test_df_3.drop(['Label.'], axis=1)\n",
    "\n",
    "x1 = df1.values.tolist()\n",
    "x2 = df2.values.tolist()\n",
    "x3 = df3.values.tolist()\n",
    "\n",
    "y1_hat = model_1.classifier.predict(x1)\n",
    "y2_hat = model_2.classifier.predict(x2)\n",
    "y3_hat = model_3.classifier.predict(x3)\n",
    "\n",
    "preds1 = []\n",
    "preds2 = []\n",
    "preds3 = []\n",
    "\n",
    "for i in y1_hat:\n",
    "    preds1.append(categories[i])\n",
    "\n",
    "for i in y2_hat:\n",
    "    preds2.append(categories[i])\n",
    "\n",
    "for i in y3_hat:\n",
    "    preds3.append(categories[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "We compute the confusion matrix to observe how well our model works on every single category in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating confusion matrix_1...\")\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    test_df_1['Label.'].replace(to_replace=category, value=i, inplace=True)\n",
    "y1_actual_ = test_df_1['Label.'].values.tolist()\n",
    "y1_actual = []\n",
    "for i in y1_actual_:\n",
    "    y1_actual.append(categories[i])\n",
    "\n",
    "print(f\"Uniques actual: {list(set(y1_actual))}\")\n",
    "print(f\"Uniques preds: {list(set(preds1))}\")\n",
    "\n",
    "y1_actu = pd.Series(y1_actual, name='Actual1')\n",
    "y1_pred = pd.Series(preds1, name='Predicted1')\n",
    "\n",
    "df_confusion1 = pd.crosstab(y1_actu, y1_pred, rownames=['Actual1'], colnames=['Predicted1'],\n",
    "                           dropna=False, margins=False, normalize='index').round(4) * 100\n",
    "\n",
    "df_confusion1.to_csv(results_path + 'rf_confusion_matrix1.csv')\n",
    "print(f\"Confusion matrix created and saved: {results_path}rf_confusion_matrix_1.csv\")\n",
    "\n",
    "print(\"#############################################################################################################\")\n",
    "\n",
    "print(\"Creating confusion matrix_2...\")\n",
    "for i, category in enumerate(categories):\n",
    "    test_df_2['Label.'].replace(to_replace=category, value=i, inplace=True)\n",
    "y2_actual_ = test_df_2['Label.'].values.tolist()\n",
    "y2_actual = []\n",
    "for i in y2_actual_:\n",
    "    y2_actual.append(categories[i])\n",
    "\n",
    "print(f\"Uniques actual: {list(set(y2_actual))}\")\n",
    "print(f\"Uniques preds: {list(set(preds2))}\")\n",
    "\n",
    "y2_actu = pd.Series(y2_actual, name='Actual2')\n",
    "y2_pred = pd.Series(preds2, name='Predicted2')\n",
    "\n",
    "df_confusion2 = pd.crosstab(y2_actu, y2_pred, rownames=['Actual2'], colnames=['Predicted2'],\n",
    "                            dropna=False, margins=False, normalize='index').round(4) * 100\n",
    "\n",
    "df_confusion2.to_csv(results_path + 'rf_confusion_matrix2.csv')\n",
    "print(f\"Confusion matrix created and saved: {results_path}rf_confusion_matrix_2.csv\")\n",
    "\n",
    "print(\"#############################################################################################################\")\n",
    "\n",
    "print(\"Creating confusion matrix_3...\")\n",
    "for i, category in enumerate(categories):\n",
    "    test_df_3['Label.'].replace(to_replace=category, value=i, inplace=True)\n",
    "y3_actual_ = test_df_3['Label.'].values.tolist()\n",
    "y3_actual = []\n",
    "for i in y3_actual_:\n",
    "    y3_actual.append(categories[i])\n",
    "\n",
    "print(f\"Uniques actual: {list(set(y3_actual))}\")\n",
    "print(f\"Uniques preds: {list(set(preds3))}\")\n",
    "\n",
    "y3_actu = pd.Series(y3_actual, name='Actual3')\n",
    "y3_pred = pd.Series(preds3, name='Predicted3')\n",
    "\n",
    "df_confusion3 = pd.crosstab(y3_actu, y3_pred, rownames=['Actual3'], colnames=['Predicted3'],\n",
    "                            dropna=False, margins=False, normalize='index').round(4) * 100\n",
    "\n",
    "df_confusion3.to_csv(results_path + 'rf_confusion_matrix3.csv')\n",
    "print(f\"Confusion matrix created and saved: {results_path}rf_confusion_matrix_3.csv\")\n",
    "\n",
    "print(\"#############################################################################################################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_confusion1\n",
    "df_confusion2\n",
    "df_confusion3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the paper, and shown by the confusion matrix above, our approach obtains a perfect classification score over the entire test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix heatmap\n",
    "\n",
    "For a clearer visualization, we plot our confusion matrix as a classification ratio for all different categories and again, observe that we have a perfect classification score on every single category in the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df1_cm =  pd.crosstab(y1_actu, y1_pred, rownames=['Actual1'], colnames=['Predicted1'],\n",
    "                           dropna=False, margins=False, normalize='index').round(4)\n",
    "plt.figure(figsize = (14,7))\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.heatmap(df1_cm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16})\n",
    "b, t = plt.ylim()\n",
    "b += 0.5\n",
    "t -= 0.5\n",
    "plt.ylim(b, t)\n",
    "plt.title('Confusion matrix 1')\n",
    "\n",
    "df2_cm =  pd.crosstab(y2_actu, y2_pred, rownames=['Actual2'], colnames=['Predicted2'],\n",
    "                            dropna=False, margins=False, normalize='index').round(4)\n",
    "plt.figure(figsize = (14,7))\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.heatmap(df2_cm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16})\n",
    "b, t = plt.ylim()\n",
    "b += 0.5\n",
    "t -= 0.5\n",
    "plt.ylim(b, t)\n",
    "plt.title('Confusion matrix 2')\n",
    "\n",
    "df3_cm =  pd.crosstab(y3_actu, y3_pred, rownames=['Actual3'], colnames=['Predicted3'],\n",
    "                            dropna=False, margins=False, normalize='index').round(4)\n",
    "plt.figure(figsize = (14,7))\n",
    "sns.set_theme(font_scale=1.4)\n",
    "sns.heatmap(df3_cm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16})\n",
    "b, t = plt.ylim()\n",
    "b += 0.5\n",
    "t -= 0.5\n",
    "plt.ylim(b, t)\n",
    "plt.title('Confusion matrix 3')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important features\n",
    "\n",
    "We can see the degree of contribution of each feature to the classification result by using the __feature\\_importances\\___ attribute of the Random Forest Classifier. We see here that, as mentioned in the paper, the 10 features extracted from our Adaptive Clustering network consistute the 10 most important features for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating feature importance scores for model_1...\")\n",
    "importances = model_1.classifier.cls.feature_importances_\n",
    "indices = np.argsort(importances)[-50:][::-1]  # -50 #most important features\n",
    "cols = test_df_1.columns.values.tolist()\n",
    "\n",
    "feature_importances_list = []\n",
    "for f in range(len(indices)):\n",
    "    feature_importances_list.append([indices[f], cols[indices[f]], importances[indices[f]]])\n",
    "\n",
    "with open(results_path + \"rf_feature_importance_scores_model_1.pkl\", \"wb\") as f:\n",
    "    pkl.dump(feature_importances_list, f)\n",
    "    print(f\"Feature importance scores saved: {results_path}rf_feature_importance_scores_model_1.pkl\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"15 most important features\")\n",
    "    pprint(feature_importances_list[:15])\n",
    "    print(\"Sum 15:\", np.sum(importances[indices[:15]]))\n",
    "    print(\"\")\n",
    "\n",
    "print(\"#############################################################################################################\")\n",
    "\n",
    "print(\"Calculating feature importance scores for model_2...\")\n",
    "importances = model_2.classifier.cls.feature_importances_\n",
    "indices = np.argsort(importances)[-50:][::-1]  # -50 #most important features\n",
    "cols = test_df_2.columns.values.tolist()\n",
    "\n",
    "feature_importances_list = []\n",
    "for f in range(len(indices)):\n",
    "    feature_importances_list.append([indices[f], cols[indices[f]], importances[indices[f]]])\n",
    "\n",
    "with open(results_path + \"rf_feature_importance_scores_model_2.pkl\", \"wb\") as f:\n",
    "    pkl.dump(feature_importances_list, f)\n",
    "    print(f\"Feature importance scores saved: {results_path}rf_feature_importance_scores_model_2.pkl\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"15 most important features\")\n",
    "    pprint(feature_importances_list[:15])\n",
    "    print(\"Sum 15:\", np.sum(importances[indices[:15]]))\n",
    "    print(\"\")\n",
    "\n",
    "print(\"#############################################################################################################\")\n",
    "\n",
    "print(\"Calculating feature importance scores for model_3...\")\n",
    "importances = model_3.classifier.cls.feature_importances_\n",
    "indices = np.argsort(importances)[-50:][::-1]  # -50 #most important features\n",
    "cols = test_df_3.columns.values.tolist()\n",
    "\n",
    "feature_importances_list = []\n",
    "for f in range(len(indices)):\n",
    "    feature_importances_list.append([indices[f], cols[indices[f]], importances[indices[f]]])\n",
    "\n",
    "with open(results_path + \"rf_feature_importance_scores_model_3.pkl\", \"wb\") as f:\n",
    "    pkl.dump(feature_importances_list, f)\n",
    "    print(f\"Feature importance scores saved: {results_path}rf_feature_importance_scores_model_3.pkl\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"15 most important features\")\n",
    "    pprint(feature_importances_list[:15])\n",
    "    print(\"Sum 15:\", np.sum(importances[indices[:15]]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics\n",
    "\n",
    "Finally, we compute the different performance metrics on our test set. We can see here again that this approach allows us to obtain a perfect score on every single evaluation metric, making this the new state-of-the-art approach for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating performance metrics for model_1...\")\n",
    "cnf_matrix = confusion_matrix(y1_actual_, y1_hat)\n",
    "_fp = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "_fn = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "_tp = np.diag(cnf_matrix)\n",
    "_tn = cnf_matrix.sum() - (_fp + _fn + _tp)\n",
    "\n",
    "_fp = _fp.astype(float)\n",
    "_fn = _fn.astype(float)\n",
    "_tp = _tp.astype(float)\n",
    "_tn = _tn.astype(float)\n",
    "\n",
    "_fp = np.mean(_fp)\n",
    "_fn = np.mean(_fn)\n",
    "_tp = np.mean(_tp)\n",
    "_tn = np.mean(_tn)\n",
    "\n",
    "metrics_handler = metrics(tp=_tp, tn=_tn, fp=_fp, fn=_fn)\n",
    "metrics_data = metrics_handler.get_metrics()\n",
    "\n",
    "accuracy_1 = accuracy_score(y1_actual_, y1_hat)\n",
    "precision, recall, f_score, support = prf(y1_actual_, y1_hat, average='weighted')\n",
    "print(\"Accuracy : {:0.7f}, Precision : {:0.7f}, Recall : {:0.7f}, F-score : {:0.7f}, FAR: {:0.7f}\"\n",
    "                                                      .format(accuracy_1, precision,\n",
    "                                                              recall, f_score, metrics_data[\"FAR\"]))\n",
    "\n",
    "with open(results_path + \"metrics_handler.pkl\", \"wb\") as f:\n",
    "    pkl.dump(metrics_handler, f)\n",
    "    print(f\"Performance metrics saved: {results_path}metrics_handler_model_1.pkl\")\n",
    "\n",
    "print(\"#############################################################################################################\")\n",
    "\n",
    "print(\"Calculating performance metrics for model_2...\")\n",
    "cnf_matrix = confusion_matrix(y2_actual_, y2_hat)\n",
    "_fp = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "_fn = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "_tp = np.diag(cnf_matrix)\n",
    "_tn = cnf_matrix.sum() - (_fp + _fn + _tp)\n",
    "\n",
    "_fp = _fp.astype(float)\n",
    "_fn = _fn.astype(float)\n",
    "_tp = _tp.astype(float)\n",
    "_tn = _tn.astype(float)\n",
    "\n",
    "_fp = np.mean(_fp)\n",
    "_fn = np.mean(_fn)\n",
    "_tp = np.mean(_tp)\n",
    "_tn = np.mean(_tn)\n",
    "\n",
    "metrics_handler = metrics(tp=_tp, tn=_tn, fp=_fp, fn=_fn)\n",
    "metrics_data = metrics_handler.get_metrics()\n",
    "\n",
    "accuracy_2 = accuracy_score(y2_actual_, y2_hat)\n",
    "precision, recall, f_score, support = prf(y2_actual_, y2_hat, average='weighted')\n",
    "\n",
    "print(\"Accuracy : {:0.7f}, Precision : {:0.7f}, Recall : {:0.7f}, F-score : {:0.7f}, FAR: {:0.7f}\"\n",
    "                                                        .format(accuracy_2, precision,\n",
    "                                                                recall, f_score, metrics_data[\"FAR\"]))\n",
    "with open(results_path + \"metrics_handler.pkl\", \"wb\") as f:\n",
    "    pkl.dump(metrics_handler, f)\n",
    "    print(f\"Performance metrics saved: {results_path}metrics_handler_model_2.pkl\")\n",
    "\n",
    "print(\"#############################################################################################################\")\n",
    "\n",
    "print(\"Calculating performance metrics for model_3...\")\n",
    "cnf_matrix = confusion_matrix(y3_actual_, y3_hat)\n",
    "_fp = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "_fn = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "_tp = np.diag(cnf_matrix)\n",
    "_tn = cnf_matrix.sum() - (_fp + _fn + _tp)\n",
    "\n",
    "_fp = _fp.astype(float)\n",
    "_fn = _fn.astype(float)\n",
    "_tp = _tp.astype(float)\n",
    "_tn = _tn.astype(float)\n",
    "\n",
    "_fp = np.mean(_fp)\n",
    "_fn = np.mean(_fn)\n",
    "_tp = np.mean(_tp)\n",
    "_tn = np.mean(_tn)\n",
    "\n",
    "metrics_handler = metrics(tp=_tp, tn=_tn, fp=_fp, fn=_fn)\n",
    "metrics_data = metrics_handler.get_metrics()\n",
    "\n",
    "accuracy_3 = accuracy_score(y3_actual_, y3_hat)\n",
    "precision, recall, f_score, support = prf(y3_actual_, y3_hat, average='weighted')\n",
    "\n",
    "print(\"Accuracy : {:0.7f}, Precision : {:0.7f}, Recall : {:0.7f}, F-score : {:0.7f}, FAR: {:0.7f}\"\n",
    "                                                        .format(accuracy_3, precision,\n",
    "                                                                recall, f_score, metrics_data[\"FAR\"]))\n",
    "with open(results_path + \"metrics_handler.pkl\", \"wb\") as f:\n",
    "    pkl.dump(metrics_handler, f)\n",
    "    print(f\"Performance metrics saved: {results_path}metrics_handler_model_3.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAPHES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couleurs = ['blue','orange','green','red']\n",
    "\"\"\"\n",
    "with open(results_path+\"jeu_de_donnees/\" +\"dic_loss.pkl\", 'rb') as file:\n",
    "    dic_loss = pkl.load(file)\n",
    "x = range(len(dic_loss[\"entrainement_1\"]['epoch_0']))\n",
    "\n",
    "plt.figure(figsize=(10, 6),facecolor='lightgrey')  # Créer une seule figure\n",
    "\n",
    "#init losses\n",
    "losses = {j: [0 for i in range(len(dic_loss[\"entrainement_1\"][\"epoch_0\"]))] for j in dic_loss[\"entrainement_1\"].keys()}\n",
    "std_loss = {j: [0 for i in range(len(dic_loss[\"entrainement_1\"][\"epoch_0\"]))] for j in dic_loss[\"entrainement_1\"].keys()}\n",
    "# print(losses)\n",
    "# print(std_loss)\n",
    "\n",
    "#Calcul de la somme des losses pour chaque tentative et à chaque epoch\n",
    "for tentative in dic_loss.keys():\n",
    "    # print(tentative)\n",
    "    for epoch in dic_loss[tentative].keys():\n",
    "        # print(epoch)\n",
    "        for v_loss in range(len(losses[epoch])):\n",
    "            # print(f\"loss courant: {losses[epoch][v_loss]} / somme: {dic_loss[tentative][epoch][v_loss]}\")\n",
    "            losses[epoch][v_loss] += dic_loss[tentative][epoch][v_loss]\n",
    "\n",
    "#Mise à jour du dico loss en mode moyenne\n",
    "losses = {j: [i/len(dic_loss.keys()) for i in losses[j]] for j in dic_loss[\"entrainement_1\"].keys()}\n",
    "# print(\"losses:\",losses)\n",
    "\n",
    "#Préparation des barres d'erreurs\n",
    "# print(\"std_loss\")\n",
    "for _,dic_epochs in dic_loss.items():\n",
    "    for e,l in dic_epochs.items():\n",
    "        # print(f\"e = {e}, l = {l}\")\n",
    "        for i in range(len(l)):\n",
    "            # print(f\"l[{i}] = {l[i]}, losses[{e}][{i}] = {losses[e][i]}\")\n",
    "            std_loss[e][i] += (l[i] - losses[e][i])**2\n",
    "            # print(std_loss[e][i])\n",
    "\n",
    "for i in std_loss.keys():\n",
    "    std_loss[i] = [np.sqrt(std_loss[i][j]/len(dic_loss.keys())) if j%50==0 else 0 for j in range(len(std_loss[i]))]\n",
    "\n",
    "c=0\n",
    "for i in losses.keys():\n",
    "    plt.errorbar(x, losses[i], yerr=std_loss[i], label=i, color=couleurs[c])\n",
    "    c+=1\n",
    "                 \n",
    "#Graphe général\n",
    "plt.xlabel('Batch number')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Evolution du loss en fonction du nombre de batch traités')\n",
    "plt.legend()\n",
    "plt.savefig(results_path + './courbes/loss_evolution_with_uncertainty.png')  # Sauvegarder la figure\n",
    "\n",
    "#Graphe précis (à partir de la 2ème epoch)\n",
    "plt.figure(figsize=(10, 6),facecolor='lightgrey')  # Créer une seule figure\n",
    "print(std_loss)\n",
    "c=0\n",
    "for i in losses.keys():\n",
    "    if i != \"epoch_0\":\n",
    "        plt.plot(x, losses[i], label=i,color=couleurs[c])\n",
    "    c+=1\n",
    "\n",
    "plt.xlabel('Batch number')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Evolution du loss en fonction du nombre de batch traités (sans la première epoch)')\n",
    "plt.legend()\n",
    "plt.savefig(results_path + './courbes/loss_evolution_epoch2.png')\n",
    "\n",
    "\n",
    "with open(results_path+\"jeu_de_donnees/\"+\"moyenne_loss.pkl\", 'wb') as file:\n",
    "        pkl.dump(losses, file)\n",
    "with open(results_path+\"jeu_de_donnees/\"+\"std_loss.pkl\", 'wb') as file:\n",
    "        pkl.dump(std_loss, file)\n",
    "\n",
    "\n",
    "#Partie émissions\n",
    "plt.figure(figsize=(10, 6),facecolor='lightgrey')  # Créer une seule figure\n",
    "liste_x = []\n",
    "liste_emissions_y = []\n",
    "liste_cpu_energy_y = []\n",
    "liste_gpu_energy_y = []\n",
    "liste_ram_energy_y = []\n",
    "liste_total_energy_y = []\n",
    "\n",
    "with open(\"emissions.csv\", \"r\") as csvfile:\n",
    "    lecteur = csv.DictReader(csvfile, delimiter=';')\n",
    "    for ligne in lecteur:\n",
    "        print(ligne)\n",
    "        if ligne['project_name'] != \"initialisation\":\n",
    "            liste_emissions_y.append(float(ligne[\"emissions (Co2 eq in kg)\"]))\n",
    "            liste_cpu_energy_y.append(float(ligne[\"cpu_energy (kWh)\"])*1000)\n",
    "            liste_gpu_energy_y.append(float(ligne[\"gpu_energy (kWh)\"])*1000)\n",
    "            liste_ram_energy_y.append(float(ligne[\"ram_energy (kWh)\"])*1000)\n",
    "            liste_total_energy_y.append(float(ligne[\"energy_consumed (kWh)\"])*1000)\n",
    "            liste_x.append(ligne['project_name'])\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(liste_x,liste_emissions_y,label=\"Emissions de CO2 (kg)\",color=\"blue\",marker='+')\n",
    "plt.xlabel('Modèle')\n",
    "plt.ylabel('Emission de CO2 (kg)')\n",
    "plt.title('Emissions de CO2 en fonction du modèle traité')\n",
    "plt.legend()\n",
    "plt.savefig(results_path + './courbes/emission_Co2.png')\n",
    "\n",
    "#partie consommation\n",
    "plt.figure(figsize=(10, 6),facecolor='lightgrey')  # Créer une seule figure\n",
    "\n",
    "plt.plot(liste_x,liste_cpu_energy_y,label=\"Energie consommée par le CPU (Wh)\",color=\"orange\",marker='+')\n",
    "plt.plot(liste_x,liste_gpu_energy_y,label=\"Energie consommée par le GPU (Wh)\",color=\"green\",marker='+')\n",
    "plt.plot(liste_x,liste_ram_energy_y,label=\"Energie consommée par la RAM (Wh)\",color=\"red\",marker='+')\n",
    "plt.plot(liste_x,liste_total_energy_y,label=\"Energie totale consommée (Wh)\",color=\"purple\",marker='+')\n",
    "\n",
    "plt.xlabel('Modèle')\n",
    "plt.ylabel('Consommation énergétique (Wh)')\n",
    "plt.title('Consommation énergétique en fonction du modèle traité')\n",
    "plt.legend()\n",
    "plt.savefig(results_path + './courbes/conso_electrique.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Conservation des données\n",
    "dico_enviro = {}\n",
    "dico_enviro[\"emissions\"] = liste_emissions_y\n",
    "dico_enviro[\"cpu_energy\"] = liste_cpu_energy_y\n",
    "dico_enviro[\"gpu_energy\"] = liste_gpu_energy_y\n",
    "dico_enviro[\"ram_energy\"] = liste_ram_energy_y\n",
    "dico_enviro[\"total_energy\"] = liste_total_energy_y\n",
    "dico_enviro[\"liste_x\"] = liste_x\n",
    "\n",
    "with open(results_path+\"jeu_de_donnees/\"+\"dico_enviro.pkl\", 'wb') as file:\n",
    "        pkl.dump(dico_enviro, file)\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import statistics as stat\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def confidence_interval(data,confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    se = st.sem(a)\n",
    "    h = se * st.t.ppf((1 + confidence) / 2.,n-1)\n",
    "    return h\n",
    "\n",
    "\n",
    "with open(results_path+\"jeu_de_donnees/\"+\"dic_loss_1.pkl\", 'rb') as file:\n",
    "    dic_loss_1 = pkl.load(file)\n",
    "with open(results_path+\"jeu_de_donnees/\"+\"dic_loss_2.pkl\", 'rb') as file:\n",
    "    dic_loss_2 = pkl.load(file)\n",
    "with open(results_path+\"jeu_de_donnees/\"+\"dic_loss_3.pkl\", 'rb') as file:\n",
    "    dic_loss_3 = pkl.load(file)\n",
    "\n",
    "liste_emissions_y = []\n",
    "liste_cpu_energy_y = []\n",
    "liste_gpu_energy_y = []\n",
    "liste_ram_energy_y = []\n",
    "liste_total_energy_y = []\n",
    "liste_x = []\n",
    "\n",
    "\n",
    "with open(\"emissions.csv\", \"r\") as csvfile:\n",
    "    lecteur = csv.DictReader(csvfile, delimiter=';')\n",
    "    for ligne in lecteur:\n",
    "        print(ligne)\n",
    "        if ligne['project_name'] != \"initialisation\":\n",
    "            liste_emissions_y.append(float(ligne[\"emissions (Co2 eq in kg)\"]))\n",
    "            liste_cpu_energy_y.append(float(ligne[\"cpu_energy (kWh)\"])*1000)\n",
    "            liste_gpu_energy_y.append(float(ligne[\"gpu_energy (kWh)\"])*1000)\n",
    "            liste_ram_energy_y.append(float(ligne[\"ram_energy (kWh)\"])*1000)\n",
    "            liste_total_energy_y.append(float(ligne[\"energy_consumed (kWh)\"])*1000)\n",
    "            liste_x.append(ligne['project_name'])\n",
    "\n",
    "\n",
    "#EVOLUTION DU LOSS MOYENNÉ\n",
    "\n",
    "x = range(len(dic_loss_1[\"entrainement_1\"]['epoch_0'])*4)\n",
    "losses = []\n",
    "\n",
    "plt.figure(figsize=(10, 6),facecolor='lightgrey')  # Créer une seule figure\n",
    "\n",
    "for i in range(len(min(dic_loss_1[\"entrainement_1\"],dic_loss_2[\"entrainement_2\"],dic_loss_3[\"entrainement_3\"]))):\n",
    "    losses += (dic_loss_1[\"entrainement_1\"][i] + dic_loss_2[\"entrainement_2\"][i] + dic_loss_3[\"entrainement_3\"][i])/3\n",
    "\n",
    "#barres d'erreurs\n",
    "std_loss = []\n",
    "transfert = {\"entraiement_1\":dic_loss_1[\"entrainement_1\"],\n",
    "             \"entrainement_2\":dic_loss_2[\"entrainement_2\"],\n",
    "             \"entrainement_3\":dic_loss_3[\"entrainement_3\"]}\n",
    "\n",
    "for i in range(len(transfert[\"entrainement_1\"])):\n",
    "    data = [transfert[j][i] for j in transfert.keys()]\n",
    "    std_loss.append(confidence_interval(data))\n",
    "\n",
    "\n",
    "plt.errorbar(x, losses, yerr=[std_loss[i] if (i%80==0) else 0 for i in range(len(losses))],label=\"Loss\",color=couleurs[0], ecolor='red')\n",
    "    \n",
    "\n",
    "with open(results_path+\"jeu_de_donnees/\"+\"moyenne_loss.pkl\", 'wb') as file:\n",
    "        pkl.dump(losses, file)\n",
    "with open(results_path+\"jeu_de_donnees/\"+\"std_loss.pkl\", 'wb') as file:\n",
    "        pkl.dump(std_loss, file)\n",
    "\n",
    "\n",
    "\n",
    "#Consommation énergétique moyenne\n",
    "\n",
    "conso_CPU = np.mean(liste_cpu_energy_y)\n",
    "with open(results_path+\"jeu_de_donnees/\"+\"conso_CPU.pkl\", 'wb') as file:\n",
    "        pkl.dump(conso_CPU, file)\n",
    "\n",
    "#ACCURACY moyenne\n",
    "\n",
    "accuracy = (accuracy_1 + accuracy_2 + accuracy_3)/3\n",
    "print(accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
